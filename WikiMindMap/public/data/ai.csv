id,href,value,name,type,content,related,tags,excerpt,hasdetailedpage
Machine Learning,#machine-learning,,,,Ability to learn without explicitly being programmed,,,,
Machine Learning.Terminology,#terminology,,,,,,,,
Machine Learning.Terminology.Workflow,#workflow,,,,,,,,
Machine Learning.Terminology.Workflow.1 Gather Data,#gather-data,,,,,,,,
Machine Learning.Terminology.Workflow.2 Explore Data,#explore-data,,,,,,,,
Machine Learning.Terminology.Workflow.2-5 Choose a Model,#choose-a-model,,,,,,,,
Machine Learning.Terminology.Workflow.3 Prepare Data,#prepare-data,,,,,,,,
Machine Learning.Terminology.Workflow.4 Build Train and Evaluate Model,#build-train-evaluate,,,,,,,,
Machine Learning.Terminology.Workflow.5 Tune Hyperparameters,#tune-hyperparameters,,,,,,,,
Machine Learning.Terminology.Workflow.6 Deploy Model,#deploy-model,,,,,,,,
Machine Learning.Terminology.Multi-Class Neural Nets,#multi-class-neural-nets,,,,,,,,
Machine Learning.Terminology.Multi-Class Neural Nets.One vs All,#one-vs-all,,,,"One vs. all provides a way to leverage binary classification. Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiers - one binary classifier for each possible outcome. During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question.<ol><li>Is this image an apple? No.</li><li>Is this image a bear? No.</li><li>Is this image candy? No.</li><li>Is this image a dog? Yes.</li><li>Is this image an egg? No.</li></ol>",,,,
Machine Learning.Terminology.Multi-Class Neural Nets.Softmax,#softmax,,,,"Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.<br>Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.<br><table><thead><tr><td>Class</td><td>Probability</td></tr></thead><tbody><tr><td>apple</td><td>0.001</td></tr><tr><td>bear</td><td>0.04</td></tr><tr><td>candy</td><td>0.008</td></tr><tr><td>dog</td><td>0.95</td></tr><tr><td>egg</td><td>0.001</td></tr></tbody></table>",,,,
Machine Learning.Terminology.Evaluation Metrics,#evaluation-metrics,,,,,,,,
Machine Learning.Terminology.Evaluation Metrics.Confusion Matrix,#confusion-matrix,,,,,,,,
Machine Learning.Terminology.Evaluation Metrics.Accuracy,#accuracy,,,Function,"Accuracy = Number of correct predictions/Total number of predictions = TP + TN / TP + TN + FP + FN <br>Where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.",,,,
Machine Learning.Terminology.Evaluation Metrics.Precision and Recall,#precision-recall,,,Function,Precision attempts to answer the following question: <br>What proportion of positive identifications was actually correct? <br>Precision = TP / TP + FP <br>Recall attempts to answer the following question: <br>What proportion of actual positives was identified correctly? <br>Recall = TP / TP + FN,,,,
Machine Learning.Terminology.Evaluation Metrics.F1 Score,#f1-score,,,Measure,"F1 is an overall measure of a model's accuracy that combines precision and recall, in that weird way that addition and multiplication just mix two ingredients to make a separate dish altogether. That is, a good F1 score means that you have low false positives and low false negatives, so you're correctly identifying real threats and you are not disturbed by false alarms. An F1 score is considered perfect when it's 1, while the model is a total failure when it's 0. <br>F1 = 2 x precision x recall / (precision + recall)",,,,
Machine Learning.Terminology.Text to vectors,#text-to-vectors,,,,,,,,
Machine Learning.Terminology.Text to vectors.Bag of words,#bag-of-words,,,,,,,,
Machine Learning.Terminology.Text to vectors.Term Frequency-Inverse Document Frequency (TF-IDF),#tf-idf,,,Algorithm,"Term-frequency-inverse document frequency (TF-IDF) is another way to judge the topic of an article by the words it contains. With TF-IDF, words are given weight - TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset. <br><img src='assets/images/tfidf.png' />",,,,
Machine Learning.Terminology.Text to vectors.word2vec,#word2vec,,,,,,,,
Machine Learning.Terminology.Decision Tree,#decision-tree,,,,"<img src='assets/images/decision_tree_nodes.png' /> <br><ul><li>Root Node: A root node is at the beginning of a tree. It represents entire population being analyzed. From the root node, the population is divided according to various features, and those sub-groups are split in turn at each decision node under the root node.</li><li>Splitting: It is a process of dividing a node into two or more sub-nodes.</li><li>Decision Node: When a sub-node splits into further sub-nodes, it's a decision node.</li><li>Leaf Node or Terminal Node: Nodes that do not split are called leaf or terminal nodes.</li><li>Pruning: Removing the sub-nodes of a parent node is called pruning. A tree is grown through splitting and shrunk through pruning.</li><li>Branch or Sub-Tree: A sub-section of decision tree is called branch or a sub-tree, just as a portion of a graph is called a sub-graph.</li><li>Parent Node and Child Node: These are relative terms. Any node that falls under another node is a child node or sub-node, and any node which precedes those child nodes is called a parent node.</li></ul>",,,,
Machine Learning.Terminology.Bayes' Theorem,#bayes-theorem,,,Formula,<img src='assets/images/bayes_theorem.jpg' /> <br><ul><li>P(A) means 'the probability that A is true.'</li><li>P(A|B) means 'the probability that A is true given that B is true.'</li></ul>,,,,
Machine Learning.Terminology.Restricted Boltzmann Machines (RBMs),#restricted-boltzmann-machines-rbm,,,Algorithm,"<img src='assets/images/two_layer_RBM.png' /><br>A Restricted Boltzmann machine is an algorithm useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning and topic modeling.<br>RBMs are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input, layer, and the second is the hidden layer.",,,,
Machine Learning.Terminology.Restricted Boltzmann Machines (RBMs).Reconstructions,#reconstructions,,,,"<img src='assets/images/reconstruction_RBM.png' /><br>The activations of hidden layer no. 1 become the input in a backward pass. They are multiplied by the same weights, one per internode edge, just as x was weight-adjusted on the forward pass. The sum of those products is added to a visible-layer bias at each visible node, and the output of those operations is a reconstruction; i.e. an approximation of the original input.",,,,
Machine Learning.Terminology.Deep-Belief Networks,#deep-belief-networks,,,Network,"A stack of restricted Boltzmann machines, in which each RBM layer communicates with both the previous and subsequent layers. The nodes of any single layer don't communicate with each other laterally.<br>This stack of RBMs might end with a a Softmax layer to create a classifier, or it may simply help cluster unlabeled data in an unsupervised learning scenario.<br>With the exception of the first and final layers, each layer in a deep-belief network has a double role: it serves as the hidden layer to the nodes that come before it, and as the input (or ""visible"") layer to the nodes that come after. It is a network built of single-layer networks.<br>Deep-belief networks are used to recognize, cluster and generate images, video sequences and motion-capture data. A continuous deep-belief network is simply an extension of a deep-belief network that accepts a continuum of decimals, rather than binary data. They were introduced by Geoff Hinton and his students in 2006.",,,,
Machine Learning.Terminology.Deep Autoencoders,#deep-autoencoders,,,,"<img src='assets/images/deep_autoencoder.png' /><br>A deep autoencoder is composed of two, symmetrical deep-belief networks that typically have four or five shallow layers representing the encoding half of the net, and second set of four or five layers that make up the decoding half.<br>The layers are restricted Boltzmann machines, the building blocks of deep-belief networks, with several peculiarities that we'll discuss below.",,,,
Machine Learning.Terminology.Deep Autoencoders.Use Cases,#deep-autoencoders-usecases,,,,,,,,
Machine Learning.Terminology.Deep Autoencoders.Use Cases.Image Search,#deep-autoencoders-image-search,,,,,,,,
Machine Learning.Terminology.Deep Autoencoders.Use Cases.Data Compression,#deep-autoencoders-data-compression,,,,,,,,
Machine Learning.Terminology.Deep Autoencoders.Use Cases.Topic Modeling & Information Retrieval (IR),#deep-autoencoders-topic-modeling-information-retrieval-ir,,,,,,,,
Machine Learning.Terminology.Differentiable Programming,#differentiable-programming,,,,<img src='assets/images/differentiable_probabilistic.jpg' />,,,,
Machine Learning.Terminology.Eigenvectors,#eigenvectors,,,,,,,,
Machine Learning.Terminology.Eigenvalues,#eigenvalues,,,,,,,,
Machine Learning.Terminology.Principal Component Analysis (PCA),#principal-component-analysis-pca,,,,,,,,
Machine Learning.Terminology.Covariance,#covariance,,,,,,,,
Machine Learning.Terminology.Entropy,#entropy,,,,,,,,
Machine Learning.Terminology.Markov Chain Monte Carlo,#markov-chain-monte-carlo,,,,,,,,
Machine Learning.Terminology.Multilayer Perceptrons (MLP),#multilayer-perceptrons-mlp,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning,#neural-network-tuning,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Data Normalization,#data-normalization,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Weight Initialization,#weight-initialization,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Epochs and Iterations,#epochs-and-iterations,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Learning Rate,#learning-rate,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Activation Function,#activation-functions,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Loss Function,#loss-function,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Regularization,#terminology-regularization,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Regularization.L2 Regularization: Simplicity,#l2-regularization,,,Technique,"A regression model that uses L2 is called Ridge Regression.<br>Ridge regression adds ""squared magnitude"" of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.<br>Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it's important how lambda is chosen. This technique works very well to avoid over-fitting issue. <br><img src='assets/images/l2-regularization-cost-function.png' />",,,,
Machine Learning.Terminology.Neural Network Tuning.Regularization.L1 Regularization: Sparsity,#l1-regularization,,,Technique,"A regression model that uses L1 regularization technique is called Lasso Regression<br>Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds ""absolute value of magnitude"" of coefficient as penalty term to the loss function.<br>Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit. <br><img src='assets/images/l1-regularization-cost-function.png' />",,,,
Machine Learning.Terminology.Neural Network Tuning.Minibatch Size,#minibatch-size,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Updater and Optimization Algorithm,#updater-and-optimization-algorithm,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Gradient Normalization,#gradient-normalization,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.RNN: Truncated BPTT,#rnn-truncated-bptt,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Deep-Belief network: Visible/Hidden Unit,#deep-belief-network-visible-hidden-unit,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.Restricted Boltzmann Machines,#tuning-restricted-boltzmann-machines,,,,,,,,
Machine Learning.Terminology.Neural Network Tuning.NaN Error,#nan-error,,,,,,,,
Machine Learning.Terminology.Tools and Platforms,#tools-and-platforms,,,,<ul>        <li><a href='https://docs.skymind.ai/docs' target='_blank'>SKIL: The Skymind Intelligence Layer</a></li>        <li><a href='https://cloud.google.com/products/machine-learning/' target='_blank'>Google Cloud AI</a></li>        <li><a href='https://aws.amazon.com/sagemaker/' target='_blank'>Amazon Sagemaker</a></li>        <li><a href='https://studio.azureml.net/' target='_blank'>Microsoft Azure Machine Learning Studio</a></li>        <li><a href='https://datascience.ibm.com/' target='_blank'>IBM's Data Science Experience (DSX)</a></li>        <li><a href='https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html' target='_blank'>Cloudera's Data Science Workbench</a></li>        <li><a href='https://docs.databricks.com/spark/latest/mllib/index.html' target='_blank'>Databricks' MLlib</a></li>        <li><a href='https://www.dominodatalab.com/' target='_blank'>Domino Datalab</a></li>        <li><a href='https://www.dataiku.com/' target='_blank'>Dataiku: Collaborative Data Science Platform</a></li>        <li><a href='https://github.com/kubeflow/kubeflow' target='_blank'>Kubeflow: Machine-Learning Toolkit for Kubernetes</a></li>        </ul>,,,,
Machine Learning.Terminology.Tools and Platforms.Apache Spark,#apache-spark,,,Software,"Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets. Spark is a distributed computing library and can run across multiple, if not hundreds, of machines.",,,,
Machine Learning.Terminology.Tools and Platforms.Arbiter - Hyperparameter Optimization,#arbiter,,,Library,"Arbiter is a library for hyperparameter optimization of neural networks. Hyperparameter optimization refers to the process of automating the selection of network hyperparameters (learning rate, number of layers, etc) in order to obtain good performance. It is part of the Deeplearning4j suite of software.<br>        Automated hyperparameter search is necessary when the optimization search is too large for a human to manually tune. Arbiter helps by doing this through multiple methods, including random search, grid search, and Bayesian methods.",,,,
Machine Learning.Terminology.Tools and Platforms.Clojure,#clojure,,,Programming Language,"Clojure is a powerful version of LISP that runs on the Java Virtual Machine and is compatible with frameworks written in other JVM languages such as Java and Scala. Below you will find a list (not comprehensive!) of tools that enable data science, machine learning and deep learning for Clojure.",,,,
Machine Learning.Terminology.Tools and Platforms.Clojure.Machine Learning,#clojure-machine-learning,,,Tools,<ul><li><a href='https://github.com/deeplearning4j/deeplearning4j'>Deeplearning4j</a></li><li><a href='https://github.com/yetanalytics/dl4clj'>dl4clj (deeplearning4j to clojure)</a></li><li><a href='https://github.com/hswick/jutsu.ai'>Jutsu.ai - Clojure wrapper for DL4J</a></li><li><a href='https://github.com/deeplearning4j/deeplearning4j/tree/master/nd4j'>ND4J: N-dimensional arrays for the JVM</a></li><li><a href='https://github.com/antoniogarrote/clj-ml'>clj-ml</a></li><li><a href='https://github.com/originrose/cortex'>cortex</a></li><li><a href='https://github.com/bigmlcom/clj-bigml'>clj-bigml</a></li><li><a href='https://github.com/rinuboney/clatern'>Clatern</a></li><li><a href='https://github.com/jimpil/enclog'>Enclog</a></li><li><a href='https://github.com/aria42/infer'>Infer</a></li><li><a href='https://github.com/gigasquid/k9'>k9</a></li><li><a href='https://github.com/cloudkj/lambda-ml'>lambda-ml</a></li><li><a href='https://github.com/clojurewerkz/statistiker'>Statistiker</a></li><li><a href='https://github.com/japonophile/synaptic'>Synaptic</a></li><li><a href='https://github.com/kieranbrowne/clojure-tensorflow'>clojure-tensorflow</a></li><li><a href='https://probprog.github.io/anglican/'>Anglican</a></li></ul>,,,,
"Machine Learning.Terminology.Tools and Platforms.Clojure.Clojure data science, scientific computing, data analysis",#clojure-data-science,,,Tools,<ul><li><a href='https://github.com/incanter/incanter'>Incanter</a></li><li><a href='http://cascalog.org/'>Cascalog</a></li><li><a href='https://github.com/onyx-platform/onyx'>Onyx</a></li><li><a href='https://github.com/gorillalabs/sparkling'>sparklling</a></li><li><a href='https://github.com/yieldbot/flambo'>flambo</a></li><li><a href='http://docs.caudate.me/lucidity/lucid-graph.html'>lucid.graph</a></li><li><a href='https://github.com/bigmlcom/histogram'>Streaming Histograms</a></li><li><a href='http://gorilla-repl.org/'>Gorilla REPL</a></li><li><a href='https://github.com/uncomplicate/bayadera'>Bayadera - Bayesian Data Analysis on the GPU</a></li><li><a href='https://github.com/uncomplicate/clojurecuda'>ClojureCUDA</a></li><li><a href='https://github.com/uncomplicate/neanderthal'>Neanderthal - fast matrix and linear algebra</a></li><li><a href='https://github.com/uncomplicate/clojurecl'>ClojureCL - parallel computations with OpenCL</a></li><li><a href='https://github.com/aysylu/loom'>Loom - graph library for Clojure</a></li></ul>,,,,
Machine Learning.Terminology.Tools and Platforms.Clojure.Computer Vision,#clojure-computer-vision,,,Tools,<ul><li><a href='https://github.com/antoniogarrote/clj-tesseract'>clj-tesseract</a></li><li><a href='http://nakkaya.com/vision.html'>vision</a></li></ul>,,,,
Machine Learning.Terminology.Tools and Platforms.Clojure.Natural Language Processing,#clojure-nlp,,,Tools,<ul><li><a href='https://github.com/dakrone/clojure-opennlp'>clojure-opennlp</a></li><li><a href='https://github.com/turbopape/postagga'>postagga</a></li></ul>,,,,
Machine Learning.Terminology.Tools and Platforms.Clojure.Parsing,#clojure-parsing,,,Tools,<ul><li><a href='https://github.com/Engelberg/instaparse'>Instaparse</a></li><li><a href='https://github.com/blancas/kern'>kern</a></li><li><a href='https://github.com/wit-ai/duckling'>duckling</a></li></ul>,,,,
Machine Learning.Terminology.Libraries,#libraries,,,,"Unless otherwise noted"", the following libraries and tools are in Python.<ul>        <li><a href='https://github.com/deeplearning4j/arbiter'>Eclipse Arbiter</a> (Java): grid and random search, and genetic search for neural architectures.</li>        <li><a href='https://github.com/automl/autoweka'>Auto-WEKA</a> (Java)</li>        <li><a href='https://github.com/rmcantin/bayesopt'>BayesOpt</a> (C++)</li>        <li><a href='https://www.featuretools.com/'>Featuretools</a>: a good library for automatically engineering features from relational and transactional data</li>        <li><a href='https://automl.github.io/auto-sklearn/stable/'>auto-sklearn</a>: a drop-in replacement for scikit-learn estimators.</li>        <li><a href='https://mlbox.readthedocs.io/en/latest/'>MLBox</a>: distributed data processing, cleaning, formatting, and state-of-the-art algorithms such as LightGBM and XGBoost. It also supports model stacking, which allows you to combine an information ensemble of models to generate a new model aiming to have better performance than the individual models.</li>        <li><a href='https://github.com/reiinakano/xcessiv'>Xcessive</a>: A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling in Python</li>        <li><a href='https://github.com/rhiever/tpot'>TPOT</a>: genetic programming to find the best performing ML pipelines,"" and it is built on top of scikit-learn</li>        <li><a href='https://github.com/tobegit3hub/advisor'>Advisor</a></li>        <li><a href='https://github.com/hyperopt/hyperopt'>Hyperopt</a></li>        <li><a href='https://github.com/hyperopt/hyperopt-sklearn'>Hyperopt-sklearn</a></li>        <li><a href='https://github.com/JasperSnoek/spearmint'>Spearmint</a></li>        <li><a href='https://github.com/automl/SMAC3'>SMAC3</a></li>        <li><a href='https://github.com/automl/RoBO'>RoBO</a></li>        <li><a href='https://github.com/fmfn/BayesianOptimization'>BayesianOptimization</a></li>        <li><a href='https://github.com/scikit-optimize/scikit-optimize'>Scikit-Optimize</a></li>        <li><a href='https://github.com/zygmuntz/hyperband'>HyperBand</a></li>        <li><a href='https://github.com/claesenm/optunity'>Optunity</a></li>        <li><a href='https://github.com/HDI-Project/ATM'>ATM</a></li>        <li><a href='https://cloud.google.com/automl/'>Cloud AutoML</a></li>        <li><a href='https://sigopt.com/'>SigOpt</a></li>        <li><a href='https://www.h2o.ai/'>H2O</a></li>        <li><a href='https://www.datarobot.com/'>DataRobot</a></li>        <li><a href='https://mljar.com/'>MLJAR</a></li>        <li><a href='http://matelabs.in/'>MateLabs</a></li>        </ul>",,,,
Machine Learning.Terminology.Frameworks,#frameworks,,,,<ul><li><a href='#torch'>Pytorch &amp; Torch</a></li><li><a href='#tensorflow'>TensorFlow</a></li><li><a href='#caffe'>Caffe</a></li><li><a href='#theano'>RIP: Theano &amp; Ecosystem</a></li><li><a href='#caffe2'>Caffe2</a></li><li><a href='#chainer'>Chainer</a></li><li><a href='#cntk'>CNTK</a></li><li><a href='#dsstne'>DSSTNE</a></li><li><a href='#dynet'>DyNet</a></li><li><a href='#gensim'>Gensim</a></li><li><a href='#gluon'>Gluon</a></li><li><a href='#keras'>Keras</a></li><li><a href='#mxnet'>Mxnet</a></li><li><a href='#paddle'>Paddle</a></li><li><a href='#bigdl'>BigDL</a></li><li><a href='#licensing'>Licensing</a></li></ul>,,,,
Machine Learning.Supervised Learning,#supervised-learning,,,,,,,,
Machine Learning.Supervised Learning.Regression,#regression,,,,,,,,
Machine Learning.Supervised Learning.Classification,#classification,,,,,,,,
Machine Learning.Supervised Learning.Classification.Thresholding,#thresholding,,,,,,,,
Machine Learning.Supervised Learning.Classification.True vs False; Positive vs Negative,#confusion-matrix,,,Matrix,"An NxN table that summarizes how successful a classification model's predictions were; that is, the correlation between the label and the model's classification. One axis of a confusion matrix is the label that the model predicted, and the other axis is the actual label. N represents the number of classes. In a binary classification problem, N=2. For example, here is a sample confusion matrix for a binary classification problem:<table><thead><tr><td></td><th>Positive (predicted)</th><th>Negative (predicted)</th></tr></thead><tbody><tr><th>Positive (actual)</th><td>1 (True Positives)</td><td>1 (False Negatives)</td></tr><tr><th>Negative (actual)</th><td>0 (False Positives)</td><td>98 (True Negatives)</td></tr></tbody></table>",,,,
Machine Learning.Supervised Learning.Classification.Accuracy,#accuracy,,,,,,,,
Machine Learning.Supervised Learning.Classification.Precision and Recall,#precision-recall,,,,,,,,
Machine Learning.Supervised Learning.Classification.ROC Curve and AUC,#roc-curve-auc,,,,,,,,
Machine Learning.Supervised Learning.Classification.Prediction Bias,#prediction-bias,,,,,,,,
Machine Learning.Unsupervised Learning,#unsupervised-learning,,,,,,,,
Machine Learning.Unsupervised Learning.Visualization,#visualization,,,,,,,,
Machine Learning.Unsupervised Learning.K-Means Clustering,#k-means-clustering,,,,,,,,
Machine Learning.Unsupervised Learning.Transfer Learning,#transfer-learning,,,,,,,,
Machine Learning.Unsupervised Learning.K-Nearest Neighbors,#k-nearest-neighbors,,,,,,,,
Machine Learning.Unsupervised Learning.VP Tree,#vp-tree,,,,,,,,
Machine Learning.Reinforcement Learning,#reinforcement-learning,,,,,,,,
Machine Learning.Reinforcement Learning.Reward,#reward,,,,,,,,
Machine Learning.Reinforcement Learning.Discount factor,#discount-factor,,,,,,,,
Machine Learning.Reinforcement Learning.Q-function,#q-function,,,,,,,,
Machine Learning.Deep Reinforcement Learning,#deep-reinforcement-learning,,,,,,,,
Machine Learning.Deep Reinforcement Learning.Algorithms,#deep-reinforcement-learning-algorithms,,,,,,,,
Machine Learning.Deep Reinforcement Learning.Algorithms.Value Learning,#value-learning,,,,,,,,
Machine Learning.Deep Reinforcement Learning.Algorithms.Policy Learning,#policy-learning,,,,,,,,
Machine Learning.Deep Reinforcement Learning.Deep Q Networks (DQN),#deep-q-networks,,,,,,,,
Machine Learning.Deep Reinforcement Learning.Policy Gradient (PG),#policy-gradient,,,,,,,,
Machine Learning.Deep Learning,#deep-learning,,,,Extract patterns from data using neural networks,,,,
Machine Learning.Deep Learning.Neural Network,#neural-network,,,,<img src='assets/images/single-layer-neural-network.png' />,,,,true
Machine Learning.Deep Learning.Perceptron,#perceptron,,The Perceptron: Forward Propagation,,<img src='assets/images/perceptron.png' />,,,,
Machine Learning.Deep Learning.Activation Functions,#activation-functions,,,,The purpose of activation functions is to introduce non-linearities into the network<br/>NOTE: All activation functions are non-linear,,,,
Machine Learning.Deep Learning.Activation Functions.Sigmoid,#sigmoid,,,Function,<img src='assets/images/sigmoid.png' />,,,,
Machine Learning.Deep Learning.Activation Functions.Hyperbolic Tangent,#hyperbolic-tangent,,,Function,<img src='assets/images/hyperbolic-tangent.png' />,,,,
Machine Learning.Deep Learning.Activation Functions.Rectified Linear Unit (ReLU),#rectified-linear-unit,,,Function,<img src='assets/images/rectified-linear-unit.png' />,,,,
Machine Learning.Deep Learning.Loss,#loss,,,,,,,,
Machine Learning.Deep Learning.Loss.Quantifying Loss,#quantifying-loss,,,Function,The loss of our network measures the cost incurred from incorrect predictions<img src='assets/images/quantifying-loss.png' />,,,,
Machine Learning.Deep Learning.Loss.Empirical Loss,#empirical-loss,,,Function,The empirical loss measures the total loss over our entire dataset<img src='assets/images/empirical-loss.png' />,,,,
Machine Learning.Deep Learning.Loss.Binary Cross Entropy Loss,#binary-cross-entropy-loss,,,Function,Cross entropy loss can be used with models that output a probability between 0 and 1 <img src='assets/images/binary-cross-entropy-loss.png' />,,,,
Machine Learning.Deep Learning.Loss.Mean Squared Error Loss,#mean-squared-error-loss,,,Function,Mean squared error loss can be used with regression models that output continuous real numbers <img src='assets/images/mean-squared-error-loss.png' />,,,,
Machine Learning.Deep Learning.Lost Function,#cost-function,,,,"Lost Function/Cost Function",,,,
Machine Learning.Deep Learning.Lost Function.Mean Squared Error,#,,,,"",,,,
Machine Learning.Deep Learning.Lost Function.Cross entropy,#,,,,"",,,,
Machine Learning.Deep Learning.Lost Function.Exponential cost,#,,,,"",,,,
Machine Learning.Deep Learning.Lost Function.Hellinger distance,#,,,,"",,,,
Machine Learning.Deep Learning.Loss Optimization,#loss-optimization,,,,Find the network weights that achieve the lowest loss <img src='assets/images/loss-optimization.png' />,,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent,#gradient-descent,,,Algorithm,"Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. <img src='assets/images/gradient-descent.png' /> <br/> <img src='assets/images/gradient-descent-algorithm.png' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent.Stochastic Gradient Descent,#stochastic-gradient-descent,,,Algorithm,"Unlike gradient descent which takes complete matrix x and complete matrix, Stochastic gradient descend takes only one training example<br>Computing gradient can be very computational to compute => Need a method to optimize <img src='assets/images/stochastic-gradient-descent.png' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent.(Mini) Batch Gradient Descent,#batch-gradient-descent,,,Algorithm,"Takes a batch, some training examples<br><img src='assets/images/artificial-intelligence/batchGradientDescent.jpg' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent.Momentum,#momentum,,,Algorithm,"<img src='assets/images/artificial-intelligence/momentum.jpg' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent.Nesterov accelerated gradient,#nesterov-accelerated-gradient,,,Algorithm,"<img src='assets/images/artificial-intelligence/NesterovAcceleratedGradient.jpg' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent.Adagrad (Adaptive Gradient),#adagrad,,,Algorithm,"<img src='assets/images/artificial-intelligence/Adagrad.jpg' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Gradient Descent.Adadelta,#adadelta,,,Algorithm,"<img src='assets/images/artificial-intelligence/Adadelta.jpg' />",,,,
Machine Learning.Deep Learning.Loss Optimization.Backpropagation,#backpropagation,,Backpropagation to computing Gradients,Algorithm,eg: How does a small change in one weight (ex. w1) affect the final loss J(W)? -> Apply chain rule <img src='assets/images/backpropagation.png' />Repeat this for every weight in the network using gradients from later layers,,,,
Machine Learning.Deep Learning.Optimization,#optimization,,,,,,,,
Machine Learning.Deep Learning.Optimization.Learning rate,#learning-rate,,,Parameter,"The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. <br>Small learning rate converges slowly and gets stuck in false local minima <br> Large learning rates overshoot, become unstable and diverge <br>Stable learning rates converge smoothly and avoid local minima",,,,
Machine Learning.Deep Learning.Optimization.Learning rate.Adaptive Learning Rate,#adaptive-learning-rate,,,,Learning rates are no longer fixed <br>Can be made larger or smaller depending on: <ul><li>how large gradient is </li><li>how fast learning is happening</li><li>size of particular weights</li><li>etc...</li></ul>,,,,
Machine Learning.Deep Learning.Optimization.Learning rate.Adaptive Learning Rate.Momentum,#adaptive-learning-rate-momentum,,,Algorithm,tf.train.MomentumOptimizer,,,,
Machine Learning.Deep Learning.Optimization.Learning rate.Adaptive Learning Rate.Adagrad,#adaptive-learning-rate-adagrad,,,Algorithm,tf.train.AdagradOptimizer,,,,
Machine Learning.Deep Learning.Optimization.Learning rate.Adaptive Learning Rate.Adadelta,#adaptive-learning-rate-adadelta,,,Algorithm,tf.train.AdadeltaOptimizer,,,,
Machine Learning.Deep Learning.Optimization.Learning rate.Adaptive Learning Rate.Adam,#adaptive-learning-rate-adam,,,Algorithm,tf.train.AdamOptimizer,,,,
Machine Learning.Deep Learning.Optimization.Learning rate.Adaptive Learning Rate.RMSProp,#adaptive-learning-rate-rmsprop,,,Algorithm,tf.train.RMSPropOptimizer,,,,
Machine Learning.Deep Learning.Optimization.Mini-batches,#mini-batches,,,,,,,,
Machine Learning.Deep Learning.Optimization.Overfitting,#overfitting,,,State,"Model is too complex, extra parameters, does not generalize well",,,,
Machine Learning.Deep Learning.Optimization.Overfitting.Regularization,#regularization,,,,,,,,
Machine Learning.Deep Learning.Optimization.Overfitting.Regularization.Regularization 1: Dropout,#regularization-dropout,,,,"During training, randomly set some activations to 0 <ul><li>Typically ""drop"" 50% of activations in layer</li><li>Forces network to not rely on any 1 node</li><li>tf.keras.layers.Dropout(p=0.5)</li></ul> <img src='assets/images/dropout.png' />",,,,
Machine Learning.Deep Learning.Optimization.Overfitting.Regularization.Regularization 2: Early Stopping,#regularization-early-stopping,,,,Stop training before we have a chance to overfit <img src='assets/images/early-stopping.png' />,,,,
Machine Learning.Applications,#applications,,,,,,,,
Machine Learning.Applications.Image Recognition,#image-recognition,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network,#convolutional-neural-network,,,Network,"<img src='assets/images/artificial-intelligence/convolutional-neural-networks.jpeg' />",,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Convolution,#convolution,,Convolution Operation,Operation,"We slide the 3x3 filter over the input image, element-wise multiply, and add the outputs: <img src='assets/images/convolution-operation.png' />",,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Non-linearity,#non-linearity,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Pooling,#pooling,,,Layer,"Max pooling<br><img src='assets/images/artificial-intelligence/MaxPooling.png' />",,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Classification: Feature Learning,#classification-feature-learning,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Classification: Class Probabilities,#classification-class-probabilities,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Training with Backpropagation,#cnn-training-with-backpropagation,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Multi Applications Architecture,#cnn-multi-applications-architecture,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Multi Applications Architecture.Classification,#cnn-classification,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Multi Applications Architecture.Semantic Segmentation,#cnn-semantic-segmentation,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Multi Applications Architecture.Semantic Segmentation.Fully Convolutional Network (FCN),#fully-convolutional-network,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Multi Applications Architecture.Object Detection,#cnn-object-detection,,,,,,,,
Machine Learning.Applications.Image Recognition.Convolutional Neural Network.Multi Applications Architecture.Image Captioning,#cnn-image-captioning,,,,,,,,
Machine Learning.Applications.Generative Modeling,#generative-modelling,,,,,,,,
Machine Learning.Applications.Generative Modeling.Problems,#generative-models-problems,,,,,,,,
Machine Learning.Applications.Generative Modeling.Problems.Debiasing,#debiasing,,,,,,,,
Machine Learning.Applications.Generative Modeling.Problems.Outlier detection,#outlier-detection,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models,#latent-variable-models,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models.Autoencoders,#autoencoders,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models.Variational Autoencoders (VAEs),#variational-autoencoders-vae,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models.Generative Adversarial Networks (GANs),#generative-adversarial-networks-gan,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models.Generative Adversarial Networks (GANs).Style-based generator,#style-based-generator,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models.Generative Adversarial Networks (GANs).Style-based transfer,#style-based-transfer,,,,,,,,
Machine Learning.Applications.Generative Modeling.Latent variable models.Generative Adversarial Networks (GANs).CycleGan: domain transformation,#cyclegan-domain-transformation,,,,,,,,
Machine Learning.Applications.Sequence Modeling,#sequence-modeling,,,,,,,,
Machine Learning.Applications.Sequence Modeling.One-hot encoding,#one-hot-encoding,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Bag of words,#bag-of-words,,,Algorithm,Bag of Words (BoW) is an algorithm that counts how many times a word appears in a document <br><img src='assets/images/wordcount-table.png' />,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs),#recurrent-neural-networks,,,,"<img src='assets/images/artificial-intelligence/RecurrentNeuralNetwork.jpg' />",,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Backpropagation through time (BPTT),#backpropagation-through-time,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Exploding gradients,#exploding-gradients,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Vanishing gradients,#vanishing-gradients,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Vanishing gradients.Rectified Linear Unit (ReLU),#relu,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Vanishing gradients.Parameter initialization,#parameter-initialization,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Vanishing gradients.Gated cells,#gated-cells,,,,,,,,
Machine Learning.Applications.Sequence Modeling.Recurrent Neural Networks (RNNs).Long Short Term Memory (LSTM),#long-short-term-memory,,,,"LSTM composed of three control neural networks, which have output vectors at time n in our sequence<br>The Long Short-Term Memory allows the model to <ul><li>forget previous data through fn</li><li>input new data though in</li><li>On controls the degree to which the memory cell goes to the output of the hidden vectors</li> <br><img src='assets/images/artificial-intelligence/LongShortTermMemory.jpg' /><br><img src='assets/images/artificial-intelligence/LongShortTermMemoryTheUpdate.jpg' />",,,,
Machine Learning.Applications.Natural Language Processing,#natural-language-processing,,,,"",,,,
Machine Learning.Applications.Natural Language Processing.Word2Vec,#word2vec,,,,"Mapping each word in the vocabulary to a vector of d-dimensional<br>Word vectors = Word embeddings<br>E.g. Map the word 'Paris' to a vector of 10 dimensions<br><img src='assets/images/artificial-intelligence/nlp.word2vec.jpg' /> ",,,,
Machine Learning.Applications.Natural Language Processing.Word2Vec.Relationships between word vectors,#relationships-between-word-vectors,,,,"The vector associated with the word king and with the word queen are essentially the same everywhere except for one component 'gender', the respective component is positive for king and negative for queen. So we're supposed to infer that this component must represent gender<br><img src='assets/images/artificial-intelligence/nlp.relationshipsBetweenWordVectors.jpg' /> ",,,,
Machine Learning.Applications.Natural Language Processing.Word2Vec.Inner products between word vectors,#inner-products-between-word-vectors,,,,"Words often have different meanings and the definition that applies corresponds to the context in which the word is used. So word2vec alone is inefficient <br><img src='assets/images/artificial-intelligence/nlp.innerproductbetweentwowords.jpg' /><br>If words W_1 and W_2 are similar, the inner product C_1. C_2 will be positive. If two words are dissimilar, that inner product, C_1, C_2 will tend to be negative ",,,,
Machine Learning.Applications.Natural Language Processing.Sentiment Analysis,#,,,,"",,,,
Machine Learning.Applications.Natural Language Processing.Recurrent Neural Networks (RNNs),#recurrent-neural-networks,,,,"",,,,
Machine Learning.Applications.Natural Language Processing.Recurrent Neural Networks (RNNs).Long Short-Term Memory (LSTM),#long-short-term-memory,,,,"",,,,
Machine Learning.Applications.Natural Language Processing.,#,,,,"",,,,
